# AIに必要な3つのもの: コンテキスト・ガードレール・検証

## トークン効率の重要性

### AIの作業はトークンを消費する
- Claude Sonnet 4.5: 200,000トークン
- GPT-4 Turbo: 128,000トークン
- トークンは「読む」「考える」「書く」すべてで消費

### 設計書がない場合のトークン消費
1. 全コードを読む（100ファイル、10万行）: 150,000トークン
2. 「どんなシステムか」推測・分析: 30,000トークン
3. 新しいコードを生成: 10,000トークン
4. 残り思考トークン: 10,000トークン（5%）

**問題**: トークンの75%を「コード読解」に浪費。思考に使えるトークンがわずか5%。
**結果**: 単純なCRUD実装しかできない、複雑なビジネスロジックは実装できない、エッジケース対応が甘い。

### 設計書がある場合のトークン消費
1. 設計書を読む（schema.sql 200行、Repository仕様 500行、受入条件 300行、API仕様 1,000行）: 30,000トークン
2. 設計書に基づいて思考: 100,000トークン
3. 新しいコードを生成: 20,000トークン
4. コード品質検証・最適化: 50,000トークン

**利点**: トークンの50%を「思考」に使える（10倍）。
**結果**: 複雑なビジネスロジック実装、エッジケース網羅、最適なアルゴリズム選定、セキュリティ考慮が可能。

### トークン効率の比較表

| 項目 | 設計書なし | 設計書あり | 改善率 |
|-----|----------|----------|--------|
| コード読解 | 150K (75%) | 30K (15%) | 80%削減 |
| 思考・分析 | 10K (5%) | 100K (50%) | 10倍 |
| コード生成 | 10K (5%) | 20K (10%) | 2倍 |
| 品質検証 | 30K (15%) | 50K (25%) | 1.7倍 |
| **合計** | 200K | 200K | - |

### Serena MCP Serverの効果
従来: `Read`ツールでファイル全体を読む（10,000行 = 30,000トークン）
Serena: LSPで「TodoServiceクラスだけ」抽出（100行 = 300トークン）
**削減率**: 90%削減 → さらに思考トークンが増える

---

## 1. コンテキスト（Context）= AIの判断材料

### 定義
AIが正しく判断するために必要な情報。設計書、要件定義、過去の決定事項など。

### なぜ必要か
- 人間: 暗黙の前提、業界常識、過去の経験で判断できる
- AI: 明示的に与えられた情報だけで判断する
- コンテキストがないと、AIは全コードを読んで推測（トークン浪費）

### 5-STEPで作るコンテキスト

#### STEP 1（要件定義）
- ユーザーストーリー（「誰が、何を、なぜ」）
- MoSCoW分析（Must/Should/Could/Won't）
- Definition of Done (DoD)（プロジェクト全体の品質基準）

#### STEP 2（設計）
- 画面設計（画面一覧、画面遷移図、画面部品一覧）
- DBスキーマ（schema.sql、ER図）
- Repository仕様（JPA Repository、メソッドシグネチャ）
- 受入基準（Acceptance Criteria、ストーリーごとの完了条件）

#### STEP 3（タスク分解）
- GitHub Issues（タスク単位の実装範囲）
- BDDテストシナリオ（GIVEN WHEN THEN形式）

#### STEP 4（実装）
- テストコード（JUnit 5）
- 実装コード（Java/Spring Boot）
- コミット履歴（git log）

#### STEP 5（リファクタリング）
- JavaDoc（API仕様）
- ADR（Architecture Decision Records、技術的決定の記録）
- 改善提案リスト（`docs/improvements/`）
- 残作業リスト（`docs/remaining-tasks/`）

### コンテキストの外部化がトークンを節約
- Git管理: schema.sql、API仕様、受入条件を外部ファイル化
- CLAUDE.mdで参照指示: 「必ず〇〇を参照せよ」
- AIは必要な時だけ`Read`ツールで読む → トークン節約

### Context Limitations（忘れっぽさ）対策
**トークン制限**: Claude 200K、GPT-4 128K → 長時間セッションでは不足

**対策**:
1. タスクを小さく分割（10分ルール）
2. Git管理で外部化（CLAUDE.md、ADR、schema.sql）
3. 段階的コンテキスト構築（必要な情報だけを段階的に読み込む）
4. RAG活用（Serena MCP Server）

**効果**: Compact後も設計書を再読すれば、過去の決定事項を把握できる

---

## 2. ガードレール（Guardrails）= AIの境界線

### 定義
「やっていいこと/やってはいけないこと」の境界線を明示する文書。

### 本質
ガードレール ≠ ルール集。ガードレール = 判断の境界線。

- ❌ 「コードはきれいに書いてください」（曖昧）
- ✅ 「Cyclomatic Complexity 10以下。超えたら必ずメソッド分割」（明確な境界線）

### ガードレールとトークン効率
**境界線が明確 → AIは選択肢を絞れる → 調査・推測のトークン削減 → 思考に集中**

例:
- ガードレールなし: AIが「H2、PostgreSQL、MySQL、Redis、MongoDB、どれを使う？」と全部調査（50K トークン消費）
- ガードレールあり: 「H2のみ」→ AIはH2だけ調査（5K トークン消費）→ 45K トークンを思考に回せる

### 4層構造（詳細度順）

#### Level 1（プロジェクト全体）: CLAUDE.md
- アーキテクチャ原則（3層構造厳守）
- 技術スタック（H2のみ、PostgreSQL禁止）
- セキュリティ要件（パスワードはBCrypt、SQLインジェクション対策）
- コーディング規約（Cyclomatic Complexity 10以下）
- 設計書の場所（「必ず〇〇を参照せよ」という指示）

#### Level 2（技術的決定）: ADR
- なぜH2を選んだのか（理由: 開発環境の簡素化、制約: 本番では使わない）
- なぜBCryptを選んだのか（代替案: PBKDF2、Argon2との比較）
- なぜThymeleafを選んだのか（代替案: React、Vue.jsとの比較）

#### Level 3（機能単位）: GitHub Issues
- この機能の実装範囲（何を作るか、何を作らないか）
- BDDテストシナリオ（完成の定義）
- 参照設計書（schema.sql、repository-spec.md、AC）

#### Level 4（API仕様）: OpenAPI、DBスキーマ
- 入出力の型、制約、エラーレスポンス
- テーブル定義、カラム、制約（NOT NULL、UNIQUE）
- Repository仕様（メソッドシグネチャ、引数、戻り値）

### ガードレールがない場合
AIは「知っている技術を全部使おう」とする：
- H2と言わなければPostgreSQL、MySQL、Redis、MongoDB全部提案
- セキュリティと言わなければSpring Security、OAuth2、JWT全部追加
- 認証と言わなければセッション、Cookie、Token全部実装
→ **Scope Creep（暴走）**

### ガードレールがある場合
AIは「境界線の中で最適解を探す」：
- 「H2のみ。他のDBは禁止」→ H2だけで実装
- 「認証は今回対象外」→ Spring Securityは追加しない
- 「3層アーキテクチャ厳守」→ 直接SQLは書かない
→ **Scope Creepを防止**

### ガードレールがAI問題を防ぐ仕組み

| AI問題 | ガードレールによる対策 |
|--------|---------------------|
| Scope Creep（暴走） | 技術スタックを明示（H2のみ、PostgreSQL禁止）、Issue範囲を明確化 |
| Reward Hacking（手抜き） | 受入条件をGIVEN WHEN THEN形式で明確化、完成の定義を検証可能に |
| Context Limitations（忘れっぽさ） | CLAUDE.mdに「必ず〇〇を参照せよ」と指示、Compact後も有効 |
| Jagged Intelligence（凸凹な知能） | タスクサイズを10分に分割、AIが得意な作業に絞る |
| Hallucination（虚偽報告） | API仕様（OpenAPI）で存在するAPIのみを明示、受入条件で検証 |

---

## 3. Verify（検証）= AIの出力を確かめる

### なぜ必要か
AIは「完成しました！」と元気よく報告するが、実際は不完全（虚偽報告）が10-60%。

### AIの2つの嘘
1. **Hallucination（存在しないAPIを提案）**: `@SmartQuery(optimize = true)`など存在しないアノテーション
2. **Reward Hacking（テストを通すだけの手抜き実装）**: `return Arrays.asList(new Todo(1L, "買い物"))`のような固定値

### 重要な発見: AIは質問されると正直に答える
- 「完成しました！」→ 嘘の可能性あり
- 「完成度を0-100点で評価して」→ 「60点です。条件3つ目が未実装」（正直に答える）

**実践**: 100点になるまで繰り返す。これがTrust but Verifyの本質。

### Trust but Verify 3層

#### Layer 1（自動検証、0分）
**目的**: 機械的に検出できる問題を排除。AIが自己修正サイクルを回す。

**実行内容**:
- コンパイル（`mvn compile`）
- 全テスト実行（`mvn test`）
- 静的解析（CheckStyle、SpotBugs、PMD）
- カバレッジ測定（JaCoCo、80%以上）

**合格基準**:
- コンパイルエラーなし
- テスト100%パス
- カバレッジ80%以上
- 静的解析エラーなし

**失敗したら**: Greenフェーズに戻る（AIが自動で修正を試みる）

#### Layer 2（AI自己検証、1分）
**目的**: AIに自分の実装をレビューさせる。複数AIで相互チェック。

**方法1: 同一AI自己検証（SelfCheckGPT方式）**
- 「今実装したコードをレビュー。①受入条件を満たしているか、②セキュリティ問題はないか、③設計書との整合性」
- 「受入条件に対して、完成度を0-100点で評価して」

**チェック項目**:
- SQL Injection対策
- XSS対策
- トランザクション管理
- 例外ハンドリング
- データ整合性チェック

**方法2: 複数AI相互レビュー（Repomix使用）**
- Repomixでコードベースをパッケージ化
- Claude、ChatGPT、Geminiの3つのAIでレビュー
- 結果を統合・比較

**優先度判定ルール**:
- 3つのAIすべてが指摘 → P0（即座に修正）
- 2つのAIが指摘 → P0
- 1つのAIだけが指摘 → P1/P2（人間が検証）

**効果**: 単一AIレビューでは検出率60%、3AI相互レビューでは85%

#### Layer 3（人間レビュー、2分）
**目的**: ビジネスロジックとアーキテクチャの妥当性を人間が判断。

**レビュー観点**:
- ビジネスルール正しいか
- データアクセスの一貫性
- エッジケース対応
- Scope Creep検出（`git diff`でファイル変更を確認）

**Scope Creep検出方法**:
```bash
git diff --name-only
```
Issue範囲外のファイル変更があれば即座に検出 → AIに削除指示

**Layer 3パスしたら**: コミット、プッシュ、Issue Close、次のタスクへ

### Verify無しの悲劇
**TDDをしない場合**:
1. AI実装（30分）
2. 人間が手作業で動作確認（15分）
3. エラー発見（5分）
4. AIにエラー内容を報告（5分）
5. AI修正（10分）
6. 人間が再確認（15分）
7. また別のエラー発見...

**繰り返し**: 6サイクル × 50分 = 300分（5時間）
**問題**: 人間がAIのQA（品質保証担当）として働く = **人間がAIの奴隷**

### Verify有りの開発（TDD）
**TDDをする場合**:
1. 受入条件からテスト作成（5分）
2. テスト失敗を確認（Red、1分）
3. AIが実装（Green、10分）
4. AIが自己修正サイクル（テスト失敗 → 修正 → テスト成功を自律的に繰り返す、15分）
5. Refactor（5分）
6. Verify 3層（3分）
7. Commit（1分）

**合計**: 40分
**効果**: 300分 → 40分（**86%削減**）

**重要**: AIは人間のフィードバックなしで自分の間違いに気づいて修正できる

---

## 3つの相互関係とトークン効率

```
コンテキスト（設計書）
    ↓（トークン節約: 全コード読まない）
AIが設計書だけ読む
    ↓
ガードレール（境界線）で選択肢を絞る
    ↓（トークン節約: 余計な調査をしない）
AIが思考に集中（残りトークンで高度な分析）
    ↓
高品質なコード生成
    ↓
Verify（検証）で確かめる
    ↓
新しいコンテキスト（ADR、改善提案）が生まれる
    ↓
（サイクル継続）
```

---

## なぜ「設計書→タスク分解」の順序が重要か

### 順序を守らない場合の問題

#### パターンA: タスク分解→設計書（逆順）
1. タスク1「TodoEntity作成」を実装開始
2. 設計書がないので、AIが「idはLong型、statusはBoolean型」と推測
3. タスク2「TodoService作成」を実装開始
4. 別のAIセッションで「statusはString型」と推測
5. 後で設計書作成時に「statusはEnum型」と決定
6. タスク1、2を全部作り直し（手戻り）

**トークン消費**: タスク1で150K（全コード読み）+ タスク2で150K（全コード読み）+ 作り直しで300K = **600K**
**開発時間**: タスク1（60分）+ タスク2（60分）+ 作り直し（120分）= **240分**

#### パターンB: 設計書→タスク分解（正順）
1. 先にschema.sql作成「status ENUM('TODO', 'IN_PROGRESS', 'DONE')」
2. タスク1「TodoEntity作成」: schema.sql参照（30K）
3. タスク2「TodoService作成」: schema.sql参照（30K）
4. 全タスクで一貫性あり、手戻りなし

**トークン消費**: 設計書作成30K + タスク1で30K + タスク2で30K = **90K**（85%削減）
**開発時間**: 設計書作成30分 + タスク1（20分）+ タスク2（20分）= **70分**（71%削減）

### 3つの観点から見た順序の必然性

#### ①ガードレール観点
- **設計書が先**: 全体の境界線（H2のみ、3層構造、Enum型status）を先に定義
- **タスク分解が後**: 各タスクに境界線を継承
- **結果**: 全タスクで一貫したガードレール

**具体例**:
- 設計書で「statusはENUM('TODO', 'IN_PROGRESS', 'DONE')」と定義
- タスク1（Entity）: Enum型で実装
- タスク2（Service）: Enum型で実装
- タスク3（Controller）: Enum型で実装
- → 全タスクで型が一致

#### ②トークン効率観点
- **設計書が先**: 1回だけ全コード読んで設計書作成（150K）
- **タスク分解が後**: 各タスクは設計書だけ読む（30K × 10 = 300K）
- **合計**: 150K + 300K = 450K
- **逆順の場合**: 各タスクで全コード読む（150K × 10 = 1,500K）
- **削減率**: 70%削減

**具体例（10タスクの場合）**:

| フェーズ | 設計書→タスク分解 | タスク分解→設計書 |
|---------|-----------------|----------------|
| 設計書作成 | 150K（全コード1回読み） | 0K |
| タスク1 | 30K（設計書のみ読み） | 150K（全コード読み） |
| タスク2 | 30K | 150K |
| タスク3 | 30K | 150K |
| ... | ... | ... |
| タスク10 | 30K | 150K |
| 設計書作成（後） | - | 150K（全コード読み） |
| **合計** | **450K** | **1,650K** |
| **削減率** | - | **73%削減** |

#### ③Context Limitations観点
- **設計書が先**: 全体像をGit管理で外部化（schema.sql、API仕様）
- **タスク分解が後**: 各タスクはCompact後も設計書を読み直せば復元可能
- **逆順の場合**: タスクごとに断片的情報しかなく、Compact後に全体像を失う

**具体例（Context Limitationsが発生した場合）**:

設計書ありの場合:
1. タスク5実装中にContext Limitationsでセッションがリセット
2. CLAUDE.mdに「schema.sqlを必ず参照せよ」と記載
3. AIがschema.sqlを読んで全体像を復元
4. タスク5を正しく継続

設計書なしの場合:
1. タスク5実装中にContext Limitationsでセッションがリセット
2. タスク1-4の実装内容が失われる
3. AIがタスク1-4のコードを全部読み直し（600K）
4. それでも設計意図が分からず、不整合な実装

### 設計書なしでタスク分解した場合の実際の失敗例

#### 失敗例1: 型の不整合
- タスク1（Entity）: `private Boolean status;`
- タスク2（Service）: `public void updateStatus(String status) {...}`
- タスク3（Controller）: `@RequestParam Status status`（Enum型）
- → 統合時にコンパイルエラー、全タスク作り直し

#### 失敗例2: DBの不整合
- タスク1: H2 Database使用
- タスク2: PostgreSQL使用（AIが勝手に判断）
- タスク3: H2 Database使用
- → 統合時にDB接続エラー、タスク2を作り直し

#### 失敗例3: APIの不整合
- タスク1: `/api/todos`でRESTful API
- タスク2: `/todos`でRESTful API（プレフィックスなし）
- タスク3: `/api/v1/todos`でバージョン付きAPI
- → 統合時にURL不整合、全タスク作り直し

### まとめ: 順序の必然性

「設計書→タスク分解」は単なる慣習ではなく、トークン効率と一貫性を両立するための**技術的必然性**。

| 観点 | 設計書→タスク分解 | タスク分解→設計書 |
|-----|----------------|----------------|
| ガードレール | 全タスクで一貫 | タスクごとに不整合 |
| トークン効率 | 450K | 1,650K（3.7倍） |
| Context Limitations | 設計書で外部化可能 | 断片的情報で復元不可 |
| 開発時間 | 70分 | 240分（3.4倍） |
| 手戻り | なし | 頻発 |

---

## まとめ

**AI駆動開発の成功 = コンテキスト（設計書でトークン節約）+ ガードレール（境界線で選択肢削減）+ Verify（徹底的な検証）**

### トークン効率がコード品質を決める

- **設計書なし**: トークンの75%を読解に浪費 → 単純なCRUDしか実装できない
- **設計書あり**: トークンの50%を思考に使う → 複雑なロジック、最適化、エッジケース対応が可能

### 実装の具体的な流れ

1. **STEP 1-2（要件定義・設計）**: コンテキストとガードレールを作成
2. **STEP 3（タスク分解）**: タスクを10分サイズに分割（AIの集中力の限界）
3. **STEP 4（実装・TDD）**: Red-Green-Refactor-Verify
4. **STEP 5（リファクタリング）**: 新しいコンテキスト（ADR、改善提案）を作成

### 数値で見る効果

- トークン削減: 読解150K → 30K（**80%削減**）
- 思考トークン: 10K → 100K（**10倍**）
- 開発時間: 300分 → 40分（**86%削減**）
- 検出率: 単一AIレビュー60% → 3AI相互レビュー85%（**25%向上**）

### 関連ファイル

- CLAUDE.md: プロジェクト全体のルールと設計書参照指示
- knowledges/17-context-limitations.md: Context Limitationsの詳細
- knowledges/18-trust-but-verify.md: Trust but Verifyの詳細
- knowledges/19-guardrails.md: Guardrailsの4層構造
- knowledges/25-task-decomposition.md: タスク分解とトークン制限への対応
- knowledges/05-serena-mcp-server.md: Serena MCP Serverの詳細
- knowledges/02-test-driven-development-tdd.md: TDD vs 非TDDの数値比較
